# -*- coding: utf-8 -*-
"""project final code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JooF1V03u8isohByoJ0F3L8Qqai31cVI
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter

from google.colab import files

# Opens a file picker dialog
uploaded = files.upload()

df = pd.read_csv("cancer patient data sets (2).csv")
df.columns = df.columns.str.strip()

print(df.head(5))

for col in df.columns:
    if df[col].dtype == "object":
        df[col] = df[col].astype("category").cat.codes

X = df.drop("Level", axis=1).values
y = df["Level"].values

def train_test_split(X, y, test_size=0.2, seed=42):
    np.random.seed(seed)
    idx = np.arange(len(X))
    np.random.shuffle(idx)
    cut = int(len(X) * (1 - test_size))
    return X[idx[:cut]], X[idx[cut:]], y[idx[:cut]], y[idx[cut:]]

X_train, X_test, y_train, y_test = train_test_split(X, y)

class KNN:
    def __init__(self, k=5):
        self.k = k
    def fit(self, X, y):
        self.X = X
        self.y = y
    def predict_one(self, x):
        distances = np.sqrt(np.sum((self.X - x)**2, axis=1))
        k_idx = distances.argsort()[:self.k]
        return Counter(self.y[k_idx]).most_common(1)[0][0]
    def predict(self, X):
        return np.array([self.predict_one(x) for x in X])

class Node:
    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value

def gini(y):
    _, counts = np.unique(y, return_counts=True)
    probs = counts / len(y)
    return 1 - np.sum(probs**2)

def best_split(X, y):
    best_gain = -1
    best_feature, best_thresh = None, None
    base = gini(y)
    for feature in range(X.shape[1]):
        for threshold in np.unique(X[:, feature]):
            left_mask = X[:, feature] <= threshold
            right_mask = ~left_mask
            if left_mask.sum() == 0 or right_mask.sum() == 0:
                continue
            g_left = gini(y[left_mask])
            g_right = gini(y[right_mask])
            gain = base - (left_mask.mean()*g_left + right_mask.mean()*g_right)
            if gain > best_gain:
                best_gain, best_feature, best_thresh = gain, feature, threshold
    return best_feature, best_thresh

def build_tree(X, y, depth=5):
    if depth == 0 or len(np.unique(y)) == 1:
        return Node(value=Counter(y).most_common(1)[0][0])
    feature, threshold = best_split(X, y)
    if feature is None:
        return Node(value=Counter(y).most_common(1)[0][0])
    left_mask = X[:, feature] <= threshold
    right_mask = ~left_mask
    left = build_tree(X[left_mask], y[left_mask], depth-1)
    right = build_tree(X[right_mask], y[right_mask], depth-1)
    return Node(feature, threshold, left, right)

def tree_predict_one(x, node):
    while node.value is None:
        if x[node.feature] <= node.threshold:
            node = node.left
        else:
            node = node.right
    return node.value

def tree_predict(X, root):
    return np.array([tree_predict_one(x, root) for x in X])

def accuracy(y_true, y_pred):
    return (y_true == y_pred).mean()

def confusion_matrix(y_true, y_pred):
    labels = np.unique(y_true)
    cm = np.zeros((len(labels), len(labels)), int)
    for t, p in zip(y_true, y_pred):
        cm[t][p] += 1
    return cm

def classification_report(cm, class_names=None):
    if class_names is None:
        class_names = [str(i) for i in range(len(cm))]
    support = cm.sum(axis=1)
    print("\nCLASSIFICATION REPORT")
    print("{:<10} {:<10} {:<10} {:<10} {:<10}".format("Class","Precision","Recall","F1","Support"))
    for i in range(len(cm)):
        tp = cm[i][i]
        fp = cm[:,i].sum() - tp
        fn = cm[i,:].sum() - tp
        p = tp/(tp+fp) if (tp+fp) else 0
        r = tp/(tp+fn) if (tp+fn) else 0
        f = 2*p*r/(p+r) if (p+r) else 0
        print("{:<10} {:<10.2f} {:<10.2f} {:<10.2f} {:<10}".format(class_names[i],p,r,f,support[i]))
    acc = np.sum(np.diag(cm))/np.sum(cm)
    print("Accuracy:", round(acc,2))

knn = KNN(k=5)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)

tree = build_tree(X_train, y_train, depth=5)
tree_pred = tree_predict(X_test, tree)

print("KNN Accuracy:", accuracy(y_test, knn_pred))
print("Decision Tree Accuracy:", accuracy(y_test, tree_pred))

cm_knn = confusion_matrix(y_test, knn_pred)
cm_tree = confusion_matrix(y_test, tree_pred)

classification_report(cm_knn, ["Low","Medium","High"])
classification_report(cm_tree, ["Low","Medium","High"])

def k_fold_split(X, y, k=5, seed=42):
    np.random.seed(seed)
    idx = np.arange(len(X))
    np.random.shuffle(idx)
    return np.array_split(idx, k)

def cross_val_knn(X, y, k=5):
    folds = k_fold_split(X, y, k)
    scores = []
    for i in range(k):
        test_idx = folds[i]
        train_idx = np.concatenate([folds[j] for j in range(k) if j != i])
        model = KNN(5)
        model.fit(X[train_idx], y[train_idx])
        pred = model.predict(X[test_idx])
        scores.append(accuracy(y[test_idx], pred))
    return scores

def cross_val_tree(X, y, k=5):
    folds = k_fold_split(X, y, k)
    scores = []
    for i in range(k):
        test_idx = folds[i]
        train_idx = np.concatenate([folds[j] for j in range(k) if j != i])
        root = build_tree(X[train_idx], y[train_idx], depth=5)
        pred = tree_predict(X[test_idx], root)
        scores.append(accuracy(y[test_idx], pred))
    return scores

knn_cv = cross_val_knn(X, y, 5)
tree_cv = cross_val_tree(X, y, 5)

print("\n===== CROSS VALIDATION RESULTS =====")
print("KNN CV Scores:", knn_cv)
print("KNN Mean:", np.mean(knn_cv))
print("Decision Tree CV Scores:", tree_cv)
print("Decision Tree Mean:", np.mean(tree_cv))

plt.figure(figsize=(6,4))
plt.plot(range(1, len(knn_cv)+1), knn_cv, marker='o')
plt.title("KNN Cross Validation Accuracy")
plt.xlabel("Fold")
plt.ylabel("Accuracy")
plt.ylim(0, 1.1)
plt.grid(True)
plt.show()

plt.figure(figsize=(6,4))
plt.plot(range(1, len(tree_cv)+1), tree_cv, marker='o')
plt.title("Decision Tree Cross Validation Accuracy")
plt.xlabel("Fold")
plt.ylabel("Accuracy")
plt.ylim(0, 1.1)
plt.grid(True)
plt.show()

# -------------------------------------------------------
# 8. CONFUSION MATRIX PLOTS
# -------------------------------------------------------

def plot_cm(cm, title, class_names):
    plt.figure(figsize=(6,5))
    plt.imshow(cm, cmap='Blues')
    plt.title(title)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")

    # Tick labels
    plt.xticks(np.arange(len(class_names)), class_names)
    plt.yticks(np.arange(len(class_names)), class_names)

    # Annotate counts
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, cm[i,j], ha='center', va='center', color='red')

    plt.colorbar()
    plt.show()

plot_cm(cm_knn, "KNN Confusion Matrix", ["Low","Medium","High"])
plot_cm(cm_tree, "Decision Tree Confusion Matrix", ["Low","Medium","High"])
